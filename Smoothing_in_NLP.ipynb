{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Smoothing Methods\n",
        "**Instructions**:\n",
        "1. Wherever you are asked to code, insert a text block below your code block and explain what you have coded as per your own understanding.\n",
        "2. If the code is provided by us, execute it, and add below a text block and provide your explanation in your own words.\n",
        "3. Submit both the .ipynb and pdf files to canvas.\n",
        "4. **The similarity score should be less than 15%.**"
      ],
      "metadata": {
        "id": "rh0LbxntRbRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task-1(15 points)\n",
        "##What is the purpose of smoothing in NLP?What are some common smoothing techniques used in NLP ?"
      ],
      "metadata": {
        "id": "oHFcy5POw0Dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here\n",
        "Smoothing is a technique used in natural language processing(NLP) to solve the problem of zero probabilities, which is occured when the language model start generating sequence of the words that its not trained before. The main aim for smoothing is to define the non zero probabilities to untrained words or n-grams so that the model can make a valuable predictions even if the input is the new.\n",
        "\n",
        "Now, we are going to talk about some smoothing techinques which is we used in NLP:\n",
        "\n",
        "1. Laplace smoothing :  Laplace smoothin also stands for additive smoothing. In this a minimum constant value will be add for count of the each word or language model in the training corpus before calculating the probabilities. This ensures that untrained words now have a non-zero probability for each occurance.\n",
        "\n",
        "2. Good-Turing smoothing: In this statistical method will be used to estimate the probability of untrained words based on the frequency of observation with similar counts. It assume that the seeing words probabilities of count c is proportional to the appear c times of frequency of the words in the training data.\n",
        "\n",
        "3. Kneser-Ney smoothing:  This technique is basically on the idea of discounting, which involves distributions some part of the probability mass from highest frequency words to lowest frequency words. this techniques use the modification process for count of each word or n-gram that take into account the number of distinct words that follows in training data.\n",
        "\n",
        "3. Interpolation smoothing :  this technique usually combine all the estinated probabilities of different kind of n-gram models to check order of initialize the probabilities to untrained words. each probabilities from each of the language model are weighted by paramter that is tuned in to the validation set.\n",
        "\n",
        "4. Backoff smoothing : This technique is used lower -order n-gram models to estimates the probability of untrained words when higher-order models fails to intialize the probability. The probability from the higher-order models are use as the fallback when the lower-order models also fail.\n"
      ],
      "metadata": {
        "id": "KbAob53ixHUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial-1(5 points)\n",
        "##Run and explain the tutorial\n",
        "##Smoothing techniques commonly used in NLP"
      ],
      "metadata": {
        "id": "zOXGZ9rOV_rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I will introduce several smoothing techniques commonly used in NLP or machine learning algorithms. They are:\n",
        "- Laplacian (add-one) Smoothing\n",
        "- Lidstone (add-k) Smoothing\n",
        "- Absolute Discounting\n",
        "- Katz Backoff\n",
        "- Kneser-Ney Smoothing\n",
        "- Interpolation"
      ],
      "metadata": {
        "id": "PLJZl5epWRME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting to implement these smoothing methods, we first need to build a *N*-gram language model, and then applying different smoothing methods to this language model, evaluating the results between these smoothing techniques. In this notebook, I will build a bigram language model."
      ],
      "metadata": {
        "id": "ijlTL8tQWVf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define some notations used in the following programs. In this notebook, **token** is the number of words in a document or a sentence, **vocab** is the number of different type of words in a document or sentence. For example, in the following sentence, there are 10 tokens and 8 vocabs (because \"I\" and \"like\" occur two times).  \n",
        "\"I like natural language processing and I like machine learning.\""
      ],
      "metadata": {
        "id": "Ca-0fh_4WY_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Language Model"
      ],
      "metadata": {
        "id": "XMu9LNDzWctP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "from numpy.random import choice\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Bigram():\n",
        "    def __init__(self):\n",
        "        self.bigram_counts = defaultdict(Counter)\n",
        "        self.unigram_counts = Counter()\n",
        "        self.context = defaultdict(Counter)\n",
        "        self.start_count = 0\n",
        "        self.token_count = 0\n",
        "        self.vocab_count = 0\n",
        "\n",
        "    def convert_sentence(self, sentence):\n",
        "        return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
        "\n",
        "    def get_counts(self, sentences):\n",
        "        # collect unigram counts\n",
        "        for sentence in sentences:\n",
        "            sentence = self.convert_sentence(sentence)\n",
        "            for word in sentence[1:]:  # from 1, because we don't need the <s> token\n",
        "                self.unigram_counts[word] += 1\n",
        "            self.start_count += 1\n",
        "\n",
        "        # collect bigram counts\n",
        "        for sentence in sentences:\n",
        "            sentence = self.convert_sentence(sentence)\n",
        "            bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "            for bigram in bigram_list:\n",
        "                self.bigram_counts[bigram[0]][bigram[1]] += 1\n",
        "                self.context[bigram[1]][bigram[0]] += 1\n",
        "        self.token_count = sum(self.unigram_counts.values())\n",
        "        self.vocab_count = len(self.unigram_counts.keys())\n",
        "\n",
        "    def generate_sentence(self):\n",
        "        current_word = \"<s>\"\n",
        "        sentence = [current_word]\n",
        "        while current_word != \"</s>\":\n",
        "            prev_word = current_word\n",
        "            prev_word_counts = self.bigram_counts[prev_word]\n",
        "            # obtain bigram probability distribution given the previous word\n",
        "            bigram_probs = []\n",
        "            total_counts = float(sum(prev_word_counts.values()))\n",
        "            for word in prev_word_counts:\n",
        "                bigram_probs.append(prev_word_counts[word] / total_counts)\n",
        "            # sample the next word\n",
        "            current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)\n",
        "            sentence.append(current_word)\n",
        "\n",
        "        sentence = \" \".join(sentence[1:-1])\n",
        "        return sentence"
      ],
      "metadata": {
        "id": "h0tIOIgJRa3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have finished building our bigram language model without any smoothing. Let's try to generate some sentences using the Penn Treebank corpora as training data.\n",
        "\n",
        "**Biagram model :** This model is calculate the  probability of word based on the previous words  based on conditional probability of one previous word.\n",
        "\n",
        "\n",
        "Biagram class : this class is initialize with different attributes: biagram_count : this is a nested defaultdict of count where the outer key is the first word in the bigram and the inner key is the second word in the bigram.\n",
        "unigram_count: this is the counter for that keeps track of the count of each words in the corpus, context is the nested defaultdict count that track of the each word that context of the another words.\n",
        "start_count : this is used for tracing the times when sentence start with''.\n",
        "\n",
        "token_count: this is used for track the total number of tokens.\n",
        "vocab_count : this is used for track the size of the vocalbulary.\n",
        "\n",
        "Now, after that we are converting sentence into the number of tokens, where the sentence is first devided in to tokens and then convert in to the lowercase using the function convert_sentence. For starting the sentence  and ending sentence  will be added. after that we are we are getting counts of list of the sentences and finally updating the value of each counts. After finicing the counts now, we are going to generate sentences using the bigram model with generate_sentence, where each sentence is start from the  generating sentence until its find the . Now, for each word we are calculating the probability based on the previous word using the bigram model counts and previuos word counts. This probability is the prediction of the next words based on the previous word. now, using the random function predict the next word based on the calculated probability. At the last generated sentence will be return.\n",
        "\n",
        "In summarize, this language model is a simple bigram language model its use a probability approach to make a new sentence depending on the bigram count in the training corpus data.\n",
        "\n"
      ],
      "metadata": {
        "id": "vFycDxhPhVOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFfge3uQRV1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a889bf00-7926-4044-9ead-b4238132a7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1\n",
            "if they can be linguistically .\n",
            "Sentence 2\n",
            "fruit .\n",
            "Sentence 3\n",
            "perhaps the purpose of origin of the surface , ran up and watched them '' .\n",
            "Sentence 4\n",
            "and efficiencies p , samuel moody .\n",
            "Sentence 5\n",
            "to insure unambiguity of the anonymous notices for livestock may take care must have fragments of their ears '' ends inherent power will steadily advancing drum song , wangled a few brief , he shook his head and productive farm homes , we have so far from any appliance and in this is our rounds of a ) , one of conscience , please '' seems to my god when you didn't they were in many of small , with household formations of ornaments and asked him , pulling the same manner of the theater tomorrow , in new bypass at the stove .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "\n",
        "bigram = Bigram()\n",
        "bigram.get_counts(brown.sents())\n",
        "for i in range(1,6):\n",
        "    print(\"Sentence %d\" % i)\n",
        "    print(bigram.generate_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focusing on this code first here imports the nltk library and after that download the brown corpus text. Additionally, here we are importing the bigram class, which is use for generate module from nltk.\n",
        "\n",
        "after that on the second task we are creating bigram class and after that in that class get count method will call based on the bigram object using the brown.sents() where brown is the corpus text which is here we are using as an argument.this method is use for counting the bigrams in the corpus.\n",
        "\n",
        "Finally, we are generating for loop where we are generating and printing the sentences here we set range for generating sentences which 1 to 6 that means 5 sentences will be generated using the generate_sentence() method of the bigram object.\n",
        "\n",
        "To conclude, this whole process is generating 5 different sentences using bigram generate_sentence method."
      ],
      "metadata": {
        "id": "r_dITBwBhziV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for our sample sentence looks reasonable, Now, let's use perplexity to evaluate different smoothing techniques at the level of the corpus. For this, we'll divide Brown corpus up randomly into a training set and a test set based on an 80/20 split. The perplexity can be calculated as follow:\n",
        "\n",
        "$PP(W) = \\sqrt[m]{\\frac{1}{P(W)}}$\n",
        "\n",
        "$\\log{PP(W)} = -\\frac{1}{m} \\log{P(W)}$"
      ],
      "metadata": {
        "id": "2AJQDI5gW6j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "def split_train_test():\n",
        "    sents = list(brown.sents())\n",
        "    shuffle(sents)\n",
        "    cutoff = int(0.8*len(sents))\n",
        "    training_set = sents[:cutoff]\n",
        "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
        "    return training_set, test_set\n",
        "\n",
        "def calculate_perplexity(sentences, bigram, smoothing_function, parameter):\n",
        "    total_log_prob = 0\n",
        "    test_token_count = 0\n",
        "    for sentence in tqdm(sentences):\n",
        "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
        "        total_log_prob += smoothing_function(sentence, bigram, parameter)\n",
        "    return math.exp(-total_log_prob / test_token_count)\n",
        "\n",
        "training_set, test_set = split_train_test()"
      ],
      "metadata": {
        "id": "Q_nbA2LSWxF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the first task is to devide the dataset in to two parts using the split_train_test(), which split the brown corpus into two different parts training and testing set. brown corpus is basically the collection of text based on american english and its majorly used for natural language processing tasks.\n",
        "\n",
        "After that we are using the shuffle function for randomly shuffle the corpus texts to check that training and test text have a proper representing sample of data.\n",
        "\n",
        "after that we are using cutoff variable for devide the sentences into parts where 80% of the senteces from the corpus, which is we are going to split and shuffle into two different parts training and test data. first 80% sentences us for training the model while other 20% are use for testing the model.Additionally, we are shuffling each of the sentences from both the parts. we are spliting the corpus text because we are evaluating the performance of the  model.our main goal is to train the model into the training set and testing the model in to the test set for checking the prediction of the model for next word into the sentences.\n",
        "\n",
        "After that we are calculating the perplexity of the model to check the model prediction for next word in sentence. if the perplexity is lower then model is giving the better performance. perplexity use the test set data as a input for the bigram model and smoothing function and smoothig parameter for testing the model.\n",
        "\n",
        "bigram model is used for calculating the probability based on the previous words and use conditional probability for one previous word for calculating the probability for the predict next word.\n",
        "\n",
        "Overall, here we are using the split method split the brown corpus into two parts and first we are training the model using the training dataset and then we are evaluating the model using the test dataset and for evaluation process we are using the perplexity which is basically calculate the predicting value for the model for next word.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S7YPBVflneru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until Now, we can evaluate the quality of different smoothing methods via calculating perplexity of test set. Now let's start to learn these smoothing techniques. For better understanding, here we use a sample example to explain these smoothing methods. Supposing we have 7 vocabs and their counts are as follows: **(Note this is a simplified example which is more like a unigram model)**\n",
        "\n",
        "vocabs | counts | unsmoothed probability\n",
        ":-: | :-: | :-:\n",
        "impropriety | 8 | 0.4 |\n",
        "offense | 5 | 0.25 |\n",
        "damage | 4 | 0.2 |\n",
        "deficiencies | 2 | 0.1 |\n",
        "outbreak | 1 | 0.05 |\n",
        "infirmity | 0 | 0 |\n",
        "cephalopods | 0 | 0 |\n",
        "**total** | **20** | **1.0** |\n",
        "\n",
        "A bigram model without any smoothing can be formulated as follow:\n",
        "$$ P(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i})}{C(w_{i-1})} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "H44nhJbTXFrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplacian (add-one) Smoothing"
      ],
      "metadata": {
        "id": "-ETsG8TAXLmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Laplacian (add-one) smoothing:\n",
        "\n",
        "$$ P_{add-1}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + 1}{C(w_{i-1}) + |V|}$$\n",
        "\n",
        "**Core idea**: Pretend that we have seen each vocab at least once."
      ],
      "metadata": {
        "id": "Wpsp-ktOXOtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabs | counts | unsmoothed probability | laplacian (add-one) smoothing\n",
        ":-: | :-: | :-: | :-:\n",
        "impropriety | 8 | 0.4 | (8+1)/(20+7)= 0.333\n",
        "offense | 5 | 0.25 | (5+1)/(20+7)= 0.222\n",
        "damage | 4 | 0.2 | (4+1)/(20+7)= 0.186\n",
        "deficiencies | 2 | 0.1 | (2+1)/(20+7)= 0.111\n",
        "outbreak | 1 | 0.05 | (1+1)/(20+7)= 0.074\n",
        "infirmity | 0 | 0 | (0+1)/(20+7)= 0.037\n",
        "cephalopods | 0 | 0 | (0+1)/(20+7)= 0.037\n",
        "**total** | **20** | **1.0** | **1.0**"
      ],
      "metadata": {
        "id": "zbT2vcVvXSBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def laplacian_smoothing(sentence, bigram, parameter):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + 1\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + len(bigram.unigram_counts)\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_laplacian_smoothing = Bigram()\n",
        "bigram_laplacian_smoothing.get_counts(training_set)\n",
        "plex_laplacian_smoothing = calculate_perplexity(test_set, bigram_laplacian_smoothing, laplacian_smoothing, None)\n",
        "print(plex_laplacian_smoothing)"
      ],
      "metadata": {
        "id": "8WJr8YctW-mJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca77ccf6-636c-411b-def1-a74ce96aa2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 28145.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3470.6520391921676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplacian smoothing:**  This technique is also known as the add-one smoothing and additive smoothing, this is basically use for smooth the probability of the words that are not occured during the training time. this is basically use for the problem zero probability, which means that if one word is not occured during the training time in the model then its give the zero probability so here its add one(1) as an probability insted of zero so the zero probability issue will be resolve.\n",
        "\n",
        "Here,in this code we are first defining the laplacian_smoothing that basically calculating the probability of the sentences using the bigram language model with laplacian smoothing. laplacian smoothin is applied to the bigram  counts to avoid the zero probability problem where if the word is not occured during the training period. this is basically done by adding 1 as the probability where the zero is occured. this probability of the bigram is now count the bigram adding the parameters devide by the sum of the counts for all bigrams that share the same the first word. for instance if the word \"The cat\" is have zero probability because its not come during the training then now changing the probability as 1 every where when the word \"the\" occured.and for that this function take 3 parameters, first is sentence where we the string is representing the evaluated sentences. bigram which is the object that represent the trained bigram language model and at the last parameter , which is the float representation of the smoothing parameter. this function first convert the input sentences into the bigram model using the convert sentence method. now we are iterating this process each time in the bigram model for the sentences and calculating the smoothing probability using the laplacian smoothing, and finally adding some log probability to the variable probability and returning the total log probability.\n",
        "\n",
        "After, that we are generating the bigram object for bigram laplacian smoothing and now traing this using the training dataset and now getting counts of the probability and we are evaluating the model using the perplexity function. now, laplacian smoothing function is used as an argument and calculate the perplexity to avoid zero probability problem and now we are printing the perplexity.\n",
        "\n",
        "All in all, we are using laplacian smoothing to avoid the zero probability issue during the testing time if the word is not known by the model during the training time."
      ],
      "metadata": {
        "id": "UkvviffzsCjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lidstone (add-k) Smoothing"
      ],
      "metadata": {
        "id": "CxfUH5qBXZJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lidstone (add-k) smoothing:\n",
        "\n",
        "$$ P_{add-k}(w_{i}|w_{i-1}) = \\frac{C(w_{i-1}, w_{i}) + k}{C(w_{i-1}) + k|V|}$$\n",
        "\n",
        "**Core idea**: Sometimes adding one is too much, instead, we add k (usually k < 1)."
      ],
      "metadata": {
        "id": "IntU_CSVXcUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabs | counts | unsmoothed probability | lidstone (add-k) smoothing (k=0.05)\n",
        ":-: | :-: | :-: | :-:\n",
        "impropriety | 8 | 0.4 | (8+0.5)/(20+7*0.5)= 0.363\n",
        "offense | 5 | 0.25 | (5+0.5)/(20+7*0.5)= 0.234\n",
        "damage | 4 | 0.2 | (4+0.5)/(20+7*0.5)= 0.191\n",
        "deficiencies | 2 | 0.1 | (2+0.5)/(20+7*0.5)= 0.106\n",
        "outbreak | 1 | 0.05 | (1+0.5)/(20+7*0.5)= 0.064\n",
        "infirmity | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
        "cephalopods | 0 | 0 | (0+0.5)/(20+7*0.5)= 0.021\n",
        "**total** | **20** | **1.0** | **1.0**"
      ],
      "metadata": {
        "id": "cuqM2gxzXf-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lidstone_smoothing(sentence, bigram, k):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word] + k\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word] + k*len(bigram.unigram_counts)\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_lidstone_smoothing = Bigram()\n",
        "bigram_lidstone_smoothing.get_counts(training_set)\n",
        "plex_lidstone_smoothing = calculate_perplexity(test_set, bigram_lidstone_smoothing, lidstone_smoothing, 0.05)\n",
        "print(plex_lidstone_smoothing)"
      ],
      "metadata": {
        "id": "bjCoy_KcXVRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fcdc2f-3e1f-4f1d-eaa3-067c6ac8bd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 28632.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1176.6756048894501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lidstone smoothing :** This is also called as the additive smoothing and laplace smoothing. this technique is used for smoothing categorical data. here we are giving the observation count for the given set with the dimensional multinomial distribution.\n",
        "\n",
        "Here, we are defining the function for lidstone smoothing and this is calculating the probability of the sentence using the lidstone smoothing for input words and smoothing parameter k.\n",
        "In this function we are passing three different parameters such as, first sentence, this is string representing the sentence for which we want to calculate the probability. second parameter is bigram, this is the bigram object for bigram language model this is containing the bigram and unigram counts of the training set.thired parameter is k, which is float represent the smoothing parameter.this function us first we are converting the sentence to the list of words using the convert sentence methof of the bigram object.this is now calculating the probability of the sentence using the lidstone smoothing formula:\n",
        "\n",
        "p(wi | wi-1) = (count(wi-1, wi) + k) / (count(wi-1) + k*V)\n",
        "\n",
        "In this formula, count(wi-1,wi) is the count of the words (wi-1,wi) from the training set, count(wi-1) this is the count of the unigram wi-1 from the training set, here V is the size of the vocabulary, and k is the smoothing parameter.\n",
        "\n",
        "this function returns the log probability of the sentence.now, we are creating the bigram class and called as a bigram lidstone smoothing, and also calls its getting counts methods to calculate the bigram and unigram count of the training dataset.\n",
        "\n",
        "To summarize, here we are calling the calculate perplexity function to calculate the preplexity of the testing dataset using the lidstone smoothing function and bigram lidstone smoothing object with a smoothing parameter.perplexity is  used for evaluating the bigram language model and using lidstone smoothing for avoid the zero probability issue. so, we can improve the prediction or evaluation value better or we can say better for predicting the next word.\n",
        "\n"
      ],
      "metadata": {
        "id": "4RtEAxRhUROO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Absolute Discounting"
      ],
      "metadata": {
        "id": "3CG4VNCIXnbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolute discounting:\n",
        "\n",
        "$$ P_{absolute-discounting}(w_{i}|w_{i-1})=\\left\\{\n",
        "\\begin{aligned}\n",
        "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
        "\\alpha(w_{i-1}) / \\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}, otherwise\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "**Core idea**: 'Borrows' a fixed probability mass from observed n-gram counts and redistributes it to unseen n-grams.\n",
        "\n",
        "$\\alpha(w_{i-1})$ is the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
        "\n",
        "$\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}$ is the count of $C(w_{i-1}, w_{j})=0$, here it is 2."
      ],
      "metadata": {
        "id": "8TwPnLiFXrDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabs | counts | unsmoothed probability | absolute discounting (d=0.1) | effective counts\n",
        ":-: | :-: | :-: | :-: | :-:\n",
        "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
        "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
        "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
        "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
        "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
        "infirmity | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
        "cephalopods | 0 | 0 | (0+0.5)/20/2=0.0125 | 0.25\n",
        "**total** | **20** | **1.0** | **1.0** | **20**"
      ],
      "metadata": {
        "id": "TAcwweytXu4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def absolute_discounting(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0:\n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0:\n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else:\n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            # count how many vocabs do not appear after pre_word\n",
        "            prev_word_discounting = bigram.vocab_count - alpha_prev_word\n",
        "            sm_bigram_counts = alpha_prev_word * d / prev_word_discounting\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_absolute_discounting = Bigram()\n",
        "bigram_absolute_discounting.get_counts(training_set)\n",
        "plex_absolute_discounting = calculate_perplexity(test_set, bigram_absolute_discounting, absolute_discounting, 0.1)\n",
        "print(plex_absolute_discounting)"
      ],
      "metadata": {
        "id": "NQ6LuXQyXjgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49df80fa-a509-4f1b-f40d-bde605ea66e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 28836.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "997.9918487270893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Absolute discounting:**  this technique is use for substract the constant discount value D, from each word where the probability is nonzero  for each word count an redistibution this probability to n-gram language model to each word with zero counts.\n",
        "\n",
        "Here, we are calculating the perplexity using the testing set using the absolute discounting smoothing techniques for bigram language model.now,  we are using the absolute discounting  function which is takes three parameter such as, sentence, bigram and d. and sentence is representing the string to evaluate. bigram is the object for the bigram language model this is contain the information of the training set, and d is the discounting factor which is use for the smoothing technique.\n",
        "\n",
        "Now, we are first converting the sentence into the list of words using the convert sentence method for the bigram object. this function now iterating for each words from the sentences and calculating the smoothing probability using the absolute discount. if the word is not available into the training set, it used the absolute discount formula  for estimating the probability. and finally we are returning  the function  with the sum of the log probability of all the words in the sentences.\n",
        "\n",
        "Here , after that we are creating the bigram object with bigram absolute discounting and call this function and its getting the counts methods with the training set to count the occurance of the words in the training set.now, we are calculating the perplexity of the function is then called using the test set, bigram absolute discounting and absolute discounting and set the value d is 0.1. this function is calculates the perplexity of the testing set using the absolute discounting smoothing technique and now returning the perplexity value.now. we are printing the perplexity using the print. and perplexity is the calculate for evaluating the model and checking that how well the model is prediction the next word of the sentences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h13ahXKBrX_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Katz Backoff"
      ],
      "metadata": {
        "id": "5xna1x5NX3ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Katz Backoff:\n",
        "\n",
        "$$ P_{backoff}(w_{i}|w_{i-1})=\\left\\{\n",
        "\\begin{aligned}\n",
        "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
        "\\alpha(w_{i-1}) \\times \\frac{P(w_{j})}{\\sum\\nolimits_{w_{j}:C(w_{i-1}, w_{j})=0}{P(w_{j})}}, otherwise\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "**Core idea**: Absolute discounting redistributes the probability mass **equally** for all unseen n-grams while Backoff redistributes the mass based on a lower order model (e.g. unigram).\n",
        "\n",
        "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.\n",
        "\n",
        "$P(w_{i})$ is the unigram probability for $w_{i}$. Suppose here $P(infirmity) = 0.002$, $P(cephalopods) = 0.008$."
      ],
      "metadata": {
        "id": "ex3zttOCX7z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabs | counts | unsmoothed probability | backoff | effective counts\n",
        ":-: | :-: | :-: | :-: | :-:\n",
        "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
        "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
        "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
        "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
        "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
        "infirmity | 0 | 0 | (0+0.5)/20*0.002/(0.002+0.008)=0.0005 | 0.1\n",
        "cephalopods | 0 | 0 | (0+0.5)/20*0.008/(0.002+0.008)=0.02 | 0.4\n",
        "**total** | **20** | **1.0** | **1.0** | **20**"
      ],
      "metadata": {
        "id": "8T_tVQKAYA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0:\n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0:\n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else:\n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "            # sum unigram counts of word j which do not appear after pre_word\n",
        "            unseen_unigram_sum = 0\n",
        "            for vocab in bigram.unigram_counts.keys():\n",
        "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
        "                    unseen_unigram_sum += bigram.unigram_counts[vocab]\n",
        "            unseen_unigram = bigram.unigram_counts[word] / unseen_unigram_sum\n",
        "            if unseen_unigram == 0: unseen_unigram = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
        "            sm_bigram_counts = alpha_prev_word * d * unseen_unigram\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_backoff = Bigram()\n",
        "bigram_backoff.get_counts(training_set)\n",
        "plex_backoff = calculate_perplexity(test_set, bigram_backoff, backoff, 0.1)\n",
        "print(plex_backoff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tkYdR_gXyOK",
        "outputId": "f0a09055-406a-4074-c0bb-3a0b6093efd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [21:40<00:00,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "580.6069065332282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**backoff smoothing:** this technique is used for getting the probability of the n-gram by re sorting the more frequent lower order n-grams. in the backoff model the probability of the language model with zero count is now set up the point by backing off to the (N-1) words.\n",
        "\n",
        "now, we are difining the fucntion called the backoff its takes three agruments, first is sentence, this represent the string as an sentece for evaluate, and also passing the bigram object for bigram language model and passing thired parameter as the d value this is a float value. now this function is cpnverting the sentence in to the list of words using the convert sentence method of the bigram object.now its calculate the probability of the sentence using the bigram language model with using the backoff smoothing.\n",
        "\n",
        "now, talking about the backoff smoothing this is technique use in the problem of the data sparsity of the language modeling. and its work as constantly reducing the order of the model until we get the non zero count is available in the training data. in this the function use the bigram model by default, but if the word count is zero then its falls back to the unigram model.\n",
        "\n",
        "This probability variable strat with zero and its increment the log probability of the each word in the sentence, and calculate the ration of the smoothing bigram count to the unsmoothing unigram count of the first word in the bigram model. the smoothing technique is performed using the discounting the parameter d and untrained word probability to assign the bigram that have  not been observed in the training data. this function returned the log probability of the sentences based on thr bigram language model with using the backoff smoothing.\n",
        " now, we are creating the bigram object using the bigram class, which is counting the bigrams and calculating the probabilities. and its call to get the count method for the bigram object to count the bigram in a training set. after that we are calling function for calculate the perplexity using the test set and bigram object for bigram language model, and then backoff function for smoothing backoff and set the value of the 0.1 for the d parameter. now we are calculating the perplexity of the test set using the language model and backoff smoothing technique. and now we are printing the perplexity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aoAqt7VczocF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kneser-Ney Smoothing"
      ],
      "metadata": {
        "id": "hG4HEdPNcgfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kneser-Ney Smoothing:\n",
        "\n",
        "$$ P_{kneser-ney-smoothing}(w_{i}|w_{i-1})=\\left\\{\n",
        "\\begin{aligned}\n",
        "\\frac{C(w_{i-1}, w_{i}) - D}{C(w_{i-1})}, if \\quad C(w_{i-1}, w_{i}) > 0 \\\\\n",
        "\\alpha(w_{i-1})P_{cont}(w_{i}), otherwise\n",
        "\\end{aligned}\n",
        "\\right.\\\\\n",
        "where \\quad\n",
        "P_{cont}(w_{i}) = \\frac{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}{{\\sum_{w_{i}}{|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|}}}\n",
        "$$\n",
        "\n",
        "**Core idea**: Redistribute probability mass based on how many number of different contexts word w has appeared in.\n",
        "\n",
        "$\\alpha(w_{i-1})$ is also the amount of probability mass that has been discounted for context $w_{i-1}$, in this example, its valuse is (0.1*5)/20.  \n",
        "Suppose we have the following phrases in the corpus: {A infirmity, B infirmity, C infirmity, D infirmity, A cephalopods}, then  \n",
        "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = infirmity is 4, $P_{cont}(w_{i}=infirmity)$ = 4/(4+1)= 0.8.  \n",
        "$|\\{w_{i-1}:C(w_{i-1}, w_{i}) > 0\\}|$ for $w_{i}$ = cephalopods is 1, $P_{cont}(w_{i}=cephalopods)$ = 1/(4+1)= 0.2"
      ],
      "metadata": {
        "id": "JFJ8M0ttcjqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabs | counts | unsmoothed probability | kneser-ney smoothing | effective counts\n",
        ":-: | :-: | :-: | :-: | :-:\n",
        "impropriety | 8 | 0.4 | (8-0.1)/20=0.395 | 7.9\n",
        "offense | 5 | 0.25 | (5-0.1)/20=0.245 | 4.9\n",
        "damage | 4 | 0.2 | (4-0.1)/20=0.195 | 3.9\n",
        "deficiencies | 2 | 0.1 | (2-0.1)/20=0.095 | 1.9\n",
        "outbreak | 1 | 0.05 | (1-0.1)/20=0.045 | 0.9\n",
        "infirmity | 0 | 0 | (0+0.5)/20*4/(4+1)=0.02 | 0.4\n",
        "cephalopods | 0 | 0 | (0+0.5)/20*1/(4+1)=0.005 | 0.1\n",
        "**total** | **20** | **1.0** | **1.0** | **20**"
      ],
      "metadata": {
        "id": "rgAh7qmScjnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kneser_ney_smoothing(sentence, bigram, d):\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if prev_word == \"<s>\": sm_unigram_counts = bigram.start_count\n",
        "        else: sm_unigram_counts = bigram.unigram_counts[prev_word]\n",
        "        if sm_unigram_counts == 0:\n",
        "            prob += math.log((1 / float(bigram.vocab_count)) * 0.01)\n",
        "            continue\n",
        "        if sm_bigram_counts != 0:\n",
        "            sm_bigram_counts = sm_bigram_counts - d\n",
        "        else:\n",
        "            # statistic how many tokens not occureed after pre_word\n",
        "            alpha_prev_word = len(bigram.bigram_counts[prev_word].keys())\n",
        "\n",
        "            context_sum = 0\n",
        "            for vocab in bigram.unigram_counts.keys():\n",
        "                if vocab not in bigram.bigram_counts[prev_word].keys():\n",
        "                    context_sum += len(bigram.context[vocab].keys())\n",
        "            p_cont = len(bigram.context[word].keys()) / context_sum\n",
        "            if p_cont == 0: p_cont = 1 / float(bigram.vocab_count - alpha_prev_word)\n",
        "            sm_bigram_counts = alpha_prev_word * d * p_cont\n",
        "        prob += math.log(sm_bigram_counts / sm_unigram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_kneser_ney_smoothing = Bigram()\n",
        "bigram_kneser_ney_smoothing.get_counts(training_set)\n",
        "plex_kneser_ney_smoothing = calculate_perplexity(test_set, bigram_kneser_ney_smoothing, kneser_ney_smoothing, 0.1)\n",
        "print(plex_kneser_ney_smoothing)"
      ],
      "metadata": {
        "id": "uEeTS4fPYEFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bd6532-cf99-4c22-8b7a-c2739f90ae72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [34:55<00:00,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "561.7911600255715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kneser–Ney smoothing:** this technique is also called as kneser-ney smoothing, this is technique is for calculating the probability of the words using the n-gram model using the given text.\n",
        " here we are defining the function kneser-ney smoothing which is implementing the kneser-ney smoothing for bigram language model and its take the parameter as an input which are sentence this is the string for sentence to be evaluate, bigram  this is the  bigram object and d is the discounting the parameter foe kneser-ney smoothing.this fucntion is the now converting the sentence into the list of words using the convert sentence method of the bigram object. now we are iterating the bigrams in the sentence, and now we are calculating the kneser-ney smoothing probability of each word.\n",
        "\n",
        "now, for the each word we are using the function to calculate the smoothing bigram count the smoothing bigram counts and also unigram count for the previous word using the smoothing unigram counts. here, suppose that unigram count is zero then the function is sets the probability of the bigram to a smallest value 0.01 which is devided by the size of the vocabulary and continues the next word.\n",
        "\n",
        "now,if the bigram count is non zero value then function applied for the kneser -ney smoothing formula to calculating the probability of the bigram language model. if the bigram count is zero then this function calculate the smoothig bigram technique for counting the probability of the word using the kneser-ney smoothing.\n",
        "\n",
        "Now , finally this function is returning the sum of the logarithms of the smoothing probabilities of all the bigram in the sentences. Now, we are creating the instance of the bigram class and  now initialize it with the training set and now we are calculating the peplexity of the test set using the calculate perplexity function. In thi technique the discounting parameter d used in the kneser-ney smoothing is set to the 0.1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4k2flymJ-Fpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpolation"
      ],
      "metadata": {
        "id": "zpHLtS1DjzFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpolation:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "P_{interpolation}(w_{i}|w_{i-1}, w_{i-2})&=\\lambda_{3}P_{3}(w_{i}|w_{i-1}, w_{i-2}) \\\\\n",
        "&+\\lambda_{2}P_{2}(w_{i}|w_{i-1})\\\\\n",
        "&+\\lambda_{1}P_{1}(w_{i})\\\\\n",
        "where \\quad\n",
        "\\sum_{i}{\\lambda_{i}} = 1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Core idea**: Combine different order n-gram models."
      ],
      "metadata": {
        "id": "7rQuz9gcj2Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolation(sentence, bigram, lambdas):\n",
        "    bigram_lambda = lambdas[0]\n",
        "    unigram_lambda = lambdas[1]\n",
        "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
        "\n",
        "    sentence = bigram.convert_sentence(sentence)\n",
        "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
        "    prob = 0\n",
        "\n",
        "    for prev_word, word in bigram_list:\n",
        "        # bigram probability\n",
        "        sm_bigram_counts = bigram.bigram_counts[prev_word][word]\n",
        "        if sm_bigram_counts == 0: interp_bigram_counts = 0\n",
        "        else:\n",
        "            if prev_word == \"<s>\": u_counts = bigram.start_count\n",
        "            else: u_counts = bigram.unigram_counts[prev_word]\n",
        "            interp_bigram_counts = sm_bigram_counts / float(u_counts) * bigram_lambda\n",
        "\n",
        "        # unigram probability\n",
        "        interp_unigram_counts = (bigram.unigram_counts[word] / bigram.token_count) * unigram_lambda\n",
        "\n",
        "        # \"zerogram\" probability: this is to account for out-of-vocabulary words, this is just 1 / |V|\n",
        "        vocab_size = len(bigram.unigram_counts)\n",
        "        interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
        "\n",
        "        prob += math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n",
        "    return prob\n",
        "\n",
        "bigram_interpolation = Bigram()\n",
        "bigram_interpolation.get_counts(training_set)\n",
        "plex_interpolation = calculate_perplexity(test_set, bigram_interpolation, interpolation, (0.8, 0.19))\n",
        "print(plex_interpolation)"
      ],
      "metadata": {
        "id": "zSpNSrtYcp6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062cc964-4ea3-4f7e-cc53-000b97631d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11468/11468 [00:00<00:00, 20678.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "431.75494032816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have finished our work, the following table shows the perplexity of different smoothing methods. We can learn that different smoothing techniques may greatly affect the quality of language models."
      ],
      "metadata": {
        "id": "9Fjb3qMqkBZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpolation:** this  is the technique which is the process of the decide the untrained value that range in between the known data points. this is use for the untrained value for any of the related data points.this technique is estimating the n-gram language probability based on the linear combination for all the lower order probabilities.\n",
        "\n",
        "Now, we are defining the function for interpolation that input takes as a sentence using the bigram model and set of lambda value as an arguments. this function calculates the probability of the sentences using the interpolation bigram, unigram and zerogram probabilities.this model is provide in the bigram object, which is containing the count of rach word and unigram in the training set. now we are converting the sentence into the list of the words ids and use to give the index into the bigram and unigram counts.\n",
        "\n",
        "Here, we are using the lambda value which is use for the weights for the contribution of each probability to the final result of the interpolation probability and set the lambda value must be sum to 1.\n",
        "\n",
        "using this function we are iterating the each words in the sentence, and now calculating the probability of the bigram,unigram and zerogram to estimate for each word. the probability of the interpolation is calculated based on the sum of the weighted probabilities and the log of the value is preventing the underflow.now, this function return the log probability of the given texts.\n",
        "\n",
        "After that we are defining the bigram object and count each words probability using the training dataset with get count method. at the last now we are calling the function for calculating the perplexity function with the test set , and use the bigram model object as well as the interpolation function additionally, the lambda value range from 0.8 to 0.19. now we are calculating the perplexity for calculate the perplexity of the testing set using the bigram model and function for the probability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HnW34RDMD4S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "smoothing techniques | perpleity on Brown test corpus\n",
        ":-: | :-:\n",
        "Laplacian (add-one) Smoothing | 3508\n",
        "Lidstone (add-k) Smoothing | 1188\n",
        "Absolute Discounting | 1013\n",
        "Katz Backoff | 588\n",
        "Kneser-Ney Smoothing | 569\n",
        "Interpolation | 436"
      ],
      "metadata": {
        "id": "OEqv55KJkD4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above implementations may not be optimal according to efficiency and memory, but it shows how different smoothing techniques work in a language model intuitively, so it may be a good tutorial for some beginners of NLP. If there are some mistakes in the code, welcome to point it out and I will correct it as soon as possible."
      ],
      "metadata": {
        "id": "NUaNfoOKkGDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task -2 (15 points)\n",
        "##Implement laplacian and lidstone smoothing on Unigram model"
      ],
      "metadata": {
        "id": "sM91PcN6kIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "\n",
        "# Get the total number of words in the corpus\n",
        "corpus = brown.words()\n",
        "print(f\"Total words in the corpus: {len(corpus)}\")\n",
        "\n",
        "# Get the first 5 sentences in the corpus\n",
        "sentences = brown.sents()[:5]\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i}: {' '.join(sentence)}\")\n",
        "\n",
        "class UnigramModel:\n",
        "    def __init__(self, corpus, alpha=0.1):\n",
        "        self.vocab = set(corpus)\n",
        "        self.word_counts = Counter(corpus)\n",
        "        self.alpha = alpha\n",
        "        self.total_words = len(corpus)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def laplacian_smoothing(self, word):\n",
        "        count = self.word_counts[word] + 1\n",
        "        return count / (self.total_words + self.vocab_size)\n",
        "\n",
        "    def lidstone_smoothing(self, word):\n",
        "        count = self.word_counts[word] + self.alpha\n",
        "        return count / (self.total_words + (self.vocab_size * self.alpha))\n",
        "\n",
        "    def unsmoothed_prob(self, word):\n",
        "        return self.word_counts[word] / self.total_words\n",
        "\n",
        "    def prob(self, word, smoothing=\"unsmoothed\"):\n",
        "        if smoothing == \"laplacian\":\n",
        "            return self.laplacian_smoothing(word)\n",
        "        elif smoothing == \"lidstone\":\n",
        "            return self.lidstone_smoothing(word)\n",
        "        else:\n",
        "            return self.unsmoothed_prob(word)\n",
        "\n",
        "    def log_prob(self, word, smoothing=\"unsmoothed\"):\n",
        "        return math.log(self.prob(word, smoothing))\n",
        "\n",
        "# Create the unigram model object\n",
        "model = UnigramModel(corpus)\n",
        "\n",
        "word = \"the\"\n",
        "print(f\"Unsmoothed probability of '{word}': {model.prob(word)}\")\n",
        "print(f\"Laplacian smoothed probability of '{word}': {model.prob(word, smoothing='laplacian')}\")\n",
        "print(f\"Lidstone smoothed probability of '{word}': {model.prob(word, smoothing='lidstone')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwcLaRxWV7t_",
        "outputId": "e56152d9-4597-494f-e7f8-9736fea1b345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in the corpus: 1161192\n",
            "Sentence 0: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
            "Sentence 1: The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
            "Sentence 2: The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
            "Sentence 3: `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
            "Sentence 4: The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
            "Unsmoothed probability of 'the': 0.05400743374050114\n",
            "Laplacian smoothed probability of 'the': 0.051521093876437773\n",
            "Lidstone smoothed probability of 'the': 0.05374804904054919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here\n",
        "Here, we are implementing the laplacian and lidstone smoothing on unigram model.and we are using the brown corpus text by natural langugae toolkit library.\n",
        "\n",
        "**Unigram model :**this is one type of language model that predicts the probability of the words based on the frequency of the corpus. In this model, each word is taken as a independent of others words in the sentece of texts. the probaility of the sentence or text is calculated by multipying the probabilities of each individual words in the sentece or text.\n",
        "\n",
        "**Laplacian smoothing :** this is techniques to define the problem of zero probabilities in a unigram model. when the word is untrained during the training time, its probability taken as a zero, which can lead the wrong predictions. Laplacian smoothing add one default(constant) value tp the each word, effectively add one count to each word.that will ensure that all each words doesn't have the probability os zero.\n",
        "\n",
        "**Lidstone smoothing :** this is more general form of laplacian smoothing its allows to add any default value to the count of each word. the constant value is mostly choose to be small, but it can be any  value. when the constant is taken as a zero then lidstone smoothing becomes laplacian smoothing.\n",
        "\n",
        "Now, first we are importing the libraries including the nltk and brown corpus, which is containing the collection of the american english text with different kind of the written english. now we are getting the words from the corpus using the len() function on the briwn.words() corpus.after that we are defining the corpus and an optional alpha parameter for lidstone smoothing. now we are initialize the vocabulary of the corpus set and now we are calculating the frequency of the word using the counter object and now we are storing the variables such as total number of word and vocalbulary size.\n",
        "\n",
        "After that we are defining the unigram model class with three smoothing methods, unsmoothing, laplacian smoothing and lidstone smoothing each of this technique is returns the estimated smoothing probability.Laplacian smoothing add count of 1 to the each word of the frequency counts, However, lidstone smoothing add a smallest positive value alpha to the counts. this probability method calculate the probability of the word using the smoothing techniques and now returns the log probability of the word.\n",
        "\n",
        "Now, we are finally creating the instance of the unigram model class using the brown corpus and calculate and then print the unsmoothing, laplacian smoothing and lidstone smoothing probability using the word \"the\" using the probability of the method with the smoothing parameters.\n",
        "\n",
        "In conclude. we are implementing the unigram language model with the three different smoothing technique and now we are applying the techniques for the corpus text to estimate the word for the probabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CmEzmWXG2qcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task - 3 (15 points)\n",
        "##What is add-k smoothing and how does it address the problem of zero probability and how it is different from kneser-Ney smoothing"
      ],
      "metadata": {
        "id": "MNakEvMYkcYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here\n",
        "Add-K smoothing is a simple smoothing technique used in language model to acknowledge the probelm of zero probability. in language model, its comman to some words or word combination are missing from the training data, which can output as the probability of zero when calculating the sentence. this is problematic because a probability of zero means that sentence is consider as impossible, even if the sentence might be plausible.\n",
        "\n",
        "Add-k smoothing involves adding small default value k to the count of each word in the training data, before normalize count to get the probability. The small constant value is choosen as the positive number , for instance 1, which is ensures that each of the word has a non-zero count, and therefore non-zero probability.\n",
        "\n",
        "The formula for calculating the smoothing probability of words its count c(w):  \n",
        "\n",
        "P(w) = (c(w) + k) / (N + kV)\n",
        "\n",
        "Where N is stands for total number of words in the training data, V is the size of the vocalbulary, k is the smoothing parameter.\n",
        "\n",
        "\n",
        "Add-k smoothing is distinct from kneser-ney smoothing in many ways.kneser-nay smoothing is a more complex smoothing technique that takes into account the frequency of the words for the sentences, rather than just individual words.its also have the discounting the probability of the high frequency of the words to give more weights to the lower frequency of the words. Kneser-Ney smoothing is majorly effective than add-k smoothing, especially for large datasets and long words of the sentences. in contrast, its also more computationally expensive and need more memory than add-k smoothing.\n",
        "\n",
        "kneser-ney smoothing is based on the idea of the discounting fector. this technique is the part of the probability now is substract from the high-frequency of the word and now we are redistributing the low frequency words. this is helping to improve the accuracy of the language model by giving more weight to the untrained n-gram language model.\n",
        "\n",
        "In contrast add-k smoothing , which is the global smoothing technique that applied the same smoothing the constant to all the events, while kneser-ney smoothing is a local smoothing technique that use the smoothing parameter for different events depends on their frequency and context. kneser-ney smoothing is more advanced and effective smoothing technique than add-k smoothing, but its more expensive for calculations.\n",
        "\n"
      ],
      "metadata": {
        "id": "-LFdey4Xk66q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task -4 (30 points)\n",
        "## Implement any Two of  the  smoothing techinques for Trigram Model and  also provide the explanation for each smoothing techniques."
      ],
      "metadata": {
        "id": "XQGiEY3dlHQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplace Smoothing:**"
      ],
      "metadata": {
        "id": "PVT23N2rTJJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"gutenberg\")\n",
        "from nltk.corpus import gutenberg\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "corpus = gutenberg.sents('shakespeare-hamlet.txt')\n",
        "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "for sentence in corpus:\n",
        "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        trigram_model[(w1, w2)][w3] += 1\n",
        "def laplace_smoothing(trigram_model, w1, w2, w3, vocabulary, k=1):\n",
        "    num = trigram_model[(w1, w2)][w3] + k\n",
        "    denom = sum(trigram_model[(w1, w2)].values()) + k * vocabulary\n",
        "    return num / denom\n",
        "vocabulary = len(set([word for sentence in corpus for word in sentence]))\n",
        "print(laplace_smoothing(trigram_model, 'to', 'be', 'or', vocabulary))\n",
        "print(laplace_smoothing(trigram_model, 'to', 'be', 'not', vocabulary))\n"
      ],
      "metadata": {
        "id": "SrOq7W-jThDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9f26e2-7244-49d5-aa23-952fbc33c550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00018278194114421494\n",
            "0.00018278194114421494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplace smoothing:** this is the simple and widely used technique to solve the zero probability problem in n-gram model. the main aim for laplace smoothing is that is to add one smallest positive constant to the count of the each words or sequences of the words, and now here distributing the probabilities into two parts one for more frequent words and one for less frequent words.\n",
        "Here, we are using the trigram model with laplace smoothing to calculate the probability of the word based on the given two first preceding words in a sentence.\n",
        "\n",
        "Here, we are first downloading the gutenberg corpus text using the nltk library and also importing the modules. after that now we are tokenizing the corpus into the sentences using the sents() method and stored them in one variable name as a corpus.\n",
        "\n",
        "Now, we are using the trigram language model and create using the defaultdict of defaultdicts. here we are using the inner defaultdict is use for store the count of the third word based on the previous two words. now we are using the outer defaultdict is use for storing the inner defaultdict so when the untrained word is find , then new defaultdict is created.\n",
        "\n",
        "After, that we are creating the function laplace smoothing with four parameters,first is trigram mode, first two words, the third word and the size of the vocabulary and the smoothing constant k as the default value 1. now, its calculate the smoothing probabilities of the third word based on the previous two words using the laplace smoothing formula.\n",
        "\n",
        "After, that we are calculating the size of the vocabulary as the number of the unique word in the corpus text, which is obtained by praising the corpus variable into the list of the words using the length of the resulting the set.\n",
        "\n",
        "At the last, we are printing the smooting probabilities of the two trigrams  using the laplace smoothing function for text \" to be or\" and \" to be not\" where the  probability for both respectively are 0.00018,0.00018.\n"
      ],
      "metadata": {
        "id": "Jwhb6ZWpTbIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Good-Turing Smoothing:**"
      ],
      "metadata": {
        "id": "47zrbBJXTO3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Load corpus text\n",
        "corpus = \"This is a sample corpus text. It contains some sentences.\"\n",
        "\n",
        "# Create trigrams from the corpus text\n",
        "trigrams = re.findall(r'\\b\\w+\\b', corpus)\n",
        "trigrams = ngrams(trigrams, 3)\n",
        "\n",
        "# Calculate the frequency of each trigram\n",
        "freq_dist = nltk.FreqDist(trigrams)\n",
        "\n",
        "# Calculate the frequency of all trigrams\n",
        "total_trigrams = freq_dist.N()\n",
        "\n",
        "# Calculate the count of trigrams that occur only once\n",
        "unique_trigrams = len([k for k, v in freq_dist.items() if v == 1])\n",
        "\n",
        "# Calculate the ratio of unique trigrams to total trigrams\n",
        "ratio = unique_trigrams / total_trigrams\n",
        "\n",
        "# Calculate smoothed probabilities for each trigram\n",
        "prob_dict = {}\n",
        "for trigram, count in freq_dist.items():\n",
        "    if count > 1:\n",
        "        prob = (freq_dist.freq(trigram) + 1) / (count + 1) * total_trigrams\n",
        "    else:\n",
        "        prob = freq_dist.freq(trigram) * ratio * total_trigrams\n",
        "    prob_dict[trigram] = prob\n",
        "\n",
        "# Estimate the probability of a trigram sequence \"this is a\"\n",
        "seq_prob = prob_dict[('This', 'is', 'a')]\n",
        "\n",
        "print(seq_prob)\n"
      ],
      "metadata": {
        "id": "63MYYO_XcR8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e951eb37-3ec5-4851-ed64-7f115dd8d0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Good-Turing smoothing :** This is the technique is use for the frequency of the wprds reallocates probability of the use for two different criteria.Here we are using the frequency of the words that only come once during the training time and total number of the bigrams for untrained words.\n",
        "\n",
        "Here, we are loading the corpus text and create the trigram  for the text using the re (regular expressions) and the ngrams() function from the natural language toolkit library.now we are calculate the frequency of each trigram using the mathematic functtion using the FreqDist() function from nltk.\n",
        "\n",
        "Here, now we are calculating the total number of trigrams of the corpus text and the number of unique words that comes only one time during the training time. now we are calculating the ratio of the unique trigram to the total trigrams.\n",
        "\n",
        "Now, we are calculating the smoothing probabilities for each word of the corpus text using the laplace smoothing. if the word is occured 1 to more times then smoothing probability is calculated based on the frequency of the trigram +1 / count of trigram + 1 * total number of trigrams. If the words occured only one time, then the smoothing probability is calculating the frequency of the trigram times.\n",
        "\n",
        "Now, we are estimating the probability of the specific word sequence \"This is a\" by checking the smoothing probability for the list of the sentences in the dictionary of probability of dict.now we are estimating the probability of the list of the ssequences and print them.\n",
        "\n",
        "In conclude, here we are calculating the smoothing probabilities of the words from the corpus text using the laplace smoothing, and now estimate the probabilities of the word sequence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yqxoqWWrlJLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Load corpus text\n",
        "corpus_text = \"This is a sample sentence. Another sentence follows.\"\n",
        "\n",
        "# Tokenize corpus text into words\n",
        "words = nltk.word_tokenize(corpus_text)\n",
        "\n",
        "# Generate trigrams\n",
        "trigrams = list(ngrams(words, 3))\n",
        "\n",
        "# Count frequency of each trigram\n",
        "trigram_freq = Counter(trigrams)\n",
        "\n",
        "# Count frequency of frequencies\n",
        "freq_freq = Counter(trigram_freq.values())\n",
        "\n",
        "# Apply Good-Turing smoothing\n",
        "N = len(trigrams)\n",
        "N1 = freq_freq[1]\n",
        "for trigram in trigram_freq:\n",
        "    count = trigram_freq[trigram]\n",
        "    if count == 0:\n",
        "        trigram_freq[trigram] = freq_freq[1] / N1\n",
        "    else:\n",
        "        trigram_freq[trigram] = (count + 1) * freq_freq[count + 1] / freq_freq[count] / N\n",
        "\n",
        "# Test the smoothed model\n",
        "test_text = \"This is another sample sentence.\"\n",
        "test_words = nltk.word_tokenize(test_text)\n",
        "test_trigrams = list(ngrams(test_words, 3))\n",
        "test_prob = 1\n",
        "for trigram in test_trigrams:\n",
        "    if trigram in trigram_freq:\n",
        "        test_prob *= trigram_freq[trigram]\n",
        "    else:\n",
        "        test_prob *= freq_freq[1] / N1\n",
        "\n",
        "print(\"Test probability:\", test_prob)\n"
      ],
      "metadata": {
        "id": "AfV-6nHXdvu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7158de2a-d838-4be8-9e32-bfb1794c5c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test probability: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Good-Turing smoothing :** This is the technique is use for the frequency of the wprds reallocates probability of the use for two different criteria.Here we are using the frequency of the words that only come once during the training time and total number of the bigrams for untrained words.\n",
        "\n",
        "Here, we are using the language model for the trigram model and use the Good-Turing smoothing technique. now we are training the language model on the corpus of the text, now they are tokenize into the each words.\n",
        "\n",
        "The trigram is the use for finding the sequence of the three words and they are generated from the tokenized word using the n-gram function from the nltk library. now we are counting the frequency of the each trigram and then use counter function from the library.\n",
        "\n",
        "Here, now we are counting the frequency of the frequency using the same counter function, here we are trying to the track the different trigrams with the same frequency.here we are eventually identified that how many words are come one time during the training, while how many times the word occure for two to more times.\n",
        "\n",
        "Now, at the next good-turing smoothing technique is applied to the trigram frequency to encounter the untrained words and adjust for the sparsity of the data. good-turing smoothing is a method of the estimating the probability of the untrained words by adjusting the frequency of the words that occured only one time or few times during the training time.\n",
        "\n",
        "now we are evaluating the smoothing model on the new text of the test set by calculating the probability of the each word in the test set dependent on the frequency of the trigrams in the training set. if the word is not fount in the text during the training time, then frequency of the word with frequency one as used as a fallback.\n",
        "\n",
        "Finally, we are printing the test probability and now indicating that how well our langauge model is predicting the next word using the smoothing technique for better performance of the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vm80xaQGTetC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Explanation\n",
        "Here, the main difference between laplace smoothing and good turing smoothing is that,  here  laplace smoothing is add a constant value of the probability to each words, whereas good turing smoothing adjust the probability of the each word based on the relative to other words. good turing smoothing is sometimes more accurate than laplace smoothing, but it required more calculating resources and a large amount of the training data.\n",
        "\n",
        "This both technique is help to improve the performace of the trigram model when dealing with sparse data and untrained data.however, which technique is use for which problem and the characteristics of the data for analyzed based on the situations.\n"
      ],
      "metadata": {
        "id": "uw5SHt8_lZqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-5(10 points)\n",
        "## Implement Laplace Smoothing for a Bigram model with varied V values and compare the effect of smoothing\n"
      ],
      "metadata": {
        "id": "ig275MkVlm4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def laplace_smoothing_bigram_model(corpus, V, alpha=1):\n",
        "    # Compute bigram counts\n",
        "    bigrams = list(nltk.bigrams(corpus))\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    unigram_counts = Counter(corpus)\n",
        "\n",
        "    # Compute bigram probabilities with Laplace smoothing\n",
        "    smoothed_probs = defaultdict(lambda: alpha/V)\n",
        "    for bigram in bigram_counts:\n",
        "        word = bigram[1]\n",
        "        count_bigram = bigram_counts[bigram]\n",
        "        count_unigram = unigram_counts[bigram[0]]\n",
        "        smoothed_prob = (count_bigram + alpha) / (count_unigram + alpha*V)\n",
        "        smoothed_probs[bigram] = smoothed_prob\n",
        "\n",
        "    return smoothed_probs\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    [\"the\", \"cat\", \"in\", \"the\", \"hat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "]\n",
        "\n",
        "# Smoothing parameter\n",
        "alpha = 1\n",
        "\n",
        "# Determine the vocabulary size dynamically based on sentence length\n",
        "for sentence in corpus:\n",
        "    # Determine sentence length\n",
        "    N = len(sentence)\n",
        "\n",
        "    # Set vocabulary size based on sentence length\n",
        "    V = N + 5\n",
        "    print(f\"Vocabulary size: {V}\")\n",
        "\n",
        "    # Compute the Laplacian-smoothed Bigram model\n",
        "    smoothed_probs = laplace_smoothing_bigram_model(sentence, V, alpha)\n",
        "\n",
        "    # Print the probabilities\n",
        "    print(f\"Sentence: {' '.join(sentence)}\")\n",
        "    for bigram in smoothed_probs:\n",
        "        print(f\"P({bigram[1]} | {bigram[0]}) = {smoothed_probs[bigram]}\")\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "id": "3foVhw2GK3AD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dcafb09-e469-43ac-e66d-8fc13e5e76ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 14\n",
            "Sentence: the cat in the hat sat on the mat\n",
            "P(cat | the) = 0.11764705882352941\n",
            "P(in | cat) = 0.13333333333333333\n",
            "P(the | in) = 0.13333333333333333\n",
            "P(hat | the) = 0.11764705882352941\n",
            "P(sat | hat) = 0.13333333333333333\n",
            "P(on | sat) = 0.13333333333333333\n",
            "P(the | on) = 0.13333333333333333\n",
            "P(mat | the) = 0.11764705882352941\n",
            "\n",
            "Vocabulary size: 14\n",
            "Sentence: the quick brown fox jumps over the lazy dog\n",
            "P(quick | the) = 0.125\n",
            "P(brown | quick) = 0.13333333333333333\n",
            "P(fox | brown) = 0.13333333333333333\n",
            "P(jumps | fox) = 0.13333333333333333\n",
            "P(over | jumps) = 0.13333333333333333\n",
            "P(the | over) = 0.13333333333333333\n",
            "P(lazy | the) = 0.125\n",
            "P(dog | lazy) = 0.13333333333333333\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Explanation\n",
        "Here, we are implementing laplacian smoothing using the bigram language model.\n",
        "the main aim for this technique is to avoid the problem of the zero probability when counting the probability of the bigram. in this bigram language model, the probability is count dependent on the basis of the previous word and its estimate the bigram divided by the count of the previous word in the corpus text. However, when the word is not available in the training set the probability count is zero and its cause the problem when trying to evaluating the model.\n",
        "\n",
        "Laplacian smoothing is add small value of the probability to all possible words, here we are ensuring that no word is having zero. This needs to be done by adding the default value to the count of the word, also here we are adding the alpha time for presenting the vocabulary size of the each previous word. the value of the alpha is based on the amount of the probability is added, and the vocabulary size defines the number of possible words size.\n",
        "\n",
        "here, we first define the laplace smoothing bigram model function which is taken the corpus text, vocabulary size of v and smoothing parameter alpha.This function first count the bigram and unigram for the corpus text and then its calculate the bigram probability using the laplace smoothing.this will returns the outpus where keys are taken as a words and values are the probability of that word and as well here we are printing the size of the vocabulary.\n",
        "\n",
        "Now, we are defining the corpus text  of two sentences, and now value of the alpha is equal to 1. now we are iterating the each sentence in to the corpus and for each sentence, now we define the sentence length using the N , and now setting the size of the vocabulary as V which is we set as dynamically which is based on the length of the sentence.now, we are counting the laplacian smoothing of bigram model using the laplace smoothing bigram model function and now we are printing the each word with probability as well as the sentence and size of the vocabulary as V.\n",
        "\n",
        "In summarize, here we are using the laplace smoothing technique for bigram model with value v for avoid zero probability problem.now, we are printing the each sentence with size of the vocab as well as the probability of the each word with each word and previous word or as a conditional probability using the previous word of the sentence."
      ],
      "metadata": {
        "id": "YmUwYLhwnDRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task - 6(10 points)\n",
        "##What are the advantages and disadvantages of interpolation smoothing and Katz backoff"
      ],
      "metadata": {
        "id": "XQ05ZcZBok82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "\n",
        "Interpolation smoothing and katz backoff are two different kind of techniques used in the natrual language processing for language models. both the techniques is used for solve the problem of data sparsity using the providing the estimated probability for untrained words during the testing time.Talking about the difference between this two technique is interpolation technique is calculating the probability and estimate it from lower-order n-grams, In contrast, katz backoff to lower-order n-grams and here its applied the discounting factor for estimating the probability for untrained data in n-grams.\n",
        "\n",
        "\n",
        "**Advantages of Interpolation smoothing:**\n",
        "1. This is simple technique which is widely used and easy to implemented.\n",
        "2. this is the result of the smoother probability and estimate the reduce of overfiting the training data.\n",
        "3. this is used with any n-gram language model and its not required to large dataset of training the data.\n",
        "4. It also improve the accuracy of the language model, when the data is sparse.\n",
        "\n",
        "\n",
        "**Disadvantages of Interpolation Smoothing:**\n",
        "1. this technique is assume that all n-gram model are equally important, here sometimes its not true for some kind of cases.\n",
        "2. it also required the setting of hyperparameters, for instance, weights for each n-gram and its difficult to tune.\n",
        "3.  its not take into account for semantic relationships between words, where the limits its effectiveness in certain application.\n",
        "\n",
        "**Advantages of Katz Backoff:**\n",
        "1. here, it takes into account for the frequency of the occurance of n-gram model, where improve the accuracy of the language model.\n",
        "2. its used with any n-gram language model and its easy to implemented.\n",
        "3. this technique is effective in reducing the impact of the dara sparsity and improving the accuracy of the language model.\n",
        "\n",
        "**Disadvantages of Katz Backoff:**\n",
        "1. this technique is more complex then interpolation technique and this technique need more calculations.\n",
        "2.  this technique is need to setting the hyperparameters, where they are difficult to tune.\n",
        "3. this technique is assume that probability of the n-gram model is independent of the probability of the sub n-grams this might be  not true in some cases.\n",
        "\n",
        "\n",
        "In coclude, this both techniques have their own advantages and disadvantages, and based on the requirement of the technique is specifies based on the task.\n",
        "Interpolation smoothing is a  simple technique and widely used and used for increasing the probability of the model.In contrast katz backoff is the more complex technique that takes into account for frequency of the occurance of n-gram and its more effective while reducing the data sparsity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEUSOBGwovH6"
      }
    }
  ]
}